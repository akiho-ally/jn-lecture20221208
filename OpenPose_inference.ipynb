{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zMMvyHhxGgtt"
   },
   "source": [
    "# 姿勢推定の実装\n",
    "\n",
    "- 本ファイルでは、学習させたOpenPoseで姿勢推定を行います。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "検出部位（関節点）\n",
    "0:鼻\n",
    "1:首\n",
    "2:右肩\n",
    "3:右肘\n",
    "4:右手首\n",
    "5:左肩\n",
    "6:左肘\n",
    "7:左手首\n",
    "8:右尻\n",
    "9:右膝\n",
    "10:右足首\n",
    "11:左尻\n",
    "12:左膝\n",
    "13:左足首\n",
    "14:右目\n",
    "15:左目\n",
    "16:右耳\n",
    "17:左耳"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hHMXPJsSGgt2"
   },
   "outputs": [],
   "source": [
    "#ライブラリをインポート\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデルの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fg_MjI-OGgt3",
    "outputId": "3cc78f55-03fb-4cf5-9541-cd242324c8b2"
   },
   "outputs": [],
   "source": [
    "from utils.openpose_net import OpenPoseNet\n",
    "\n",
    "# 学習済みモデルと本章のモデルでネットワークの層の名前が違うので、対応させてロードする\n",
    "# モデルの定義\n",
    "net = OpenPoseNet()\n",
    "\n",
    "# 学習済みパラメータをロードする\n",
    "net_weights = torch.load(\n",
    "    './weights/pose_model.pth', map_location={'cuda:0': 'cpu'})\n",
    "keys = list(net_weights.keys())\n",
    "\n",
    "weights_load = {}\n",
    "\n",
    "# ロードした内容を、構築したモデルのパラメータ名net.state_dict().keys()にコピーする\n",
    "for i in range(len(keys)):\n",
    "    weights_load[list(net.state_dict().keys())[i]\n",
    "                 ] = net_weights[list(keys)[i]]\n",
    "\n",
    "# コピーした内容をモデルに与える\n",
    "state = net.state_dict()\n",
    "state.update(weights_load)\n",
    "net.load_state_dict(state)\n",
    "\n",
    "print('ネットワーク設定完了：学習済みの重みをロードしました')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データの準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9xX2RJt_Ggt4",
    "outputId": "610eadec-b0e9-4d3e-ca1c-92ff8479e60f"
   },
   "outputs": [],
   "source": [
    "# 画像を読み込み、前処理します\n",
    "\n",
    "test_image = './data/image.jpg'  #宿題ではここに集合写真のパスを指定してください\n",
    "oriImg = cv2.imread(test_image)  # B,G,Rの順番\n",
    "\n",
    "# BGRをRGBにして表示\n",
    "oriImg = cv2.cvtColor(oriImg, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(oriImg)\n",
    "plt.show()\n",
    "\n",
    "# 画像のリサイズ\n",
    "size = (368, 368)\n",
    "img = cv2.resize(oriImg, size, interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "# 画像の前処理\n",
    "img = img.astype(np.float32) / 255.\n",
    "\n",
    "# 色情報の標準化\n",
    "color_mean = [0.485, 0.456, 0.406]\n",
    "color_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "preprocessed_img = img.copy()  # RGB\n",
    "\n",
    "for i in range(3):\n",
    "    preprocessed_img[:, :, i] = preprocessed_img[:, :, i] - color_mean[i]\n",
    "    preprocessed_img[:, :, i] = preprocessed_img[:, :, i] / color_std[i]\n",
    "\n",
    "# （高さ、幅、色）→（色、高さ、幅）\n",
    "img = preprocessed_img.transpose((2, 0, 1)).astype(np.float32)\n",
    "\n",
    "# 画像をTensorに\n",
    "img = torch.from_numpy(img)\n",
    "\n",
    "# ミニバッチ化：torch.Size([1, 3, 368, 368])\n",
    "x = img.unsqueeze(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confidence MapとPAFsを求める"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gRwwD0jTGgt4"
   },
   "outputs": [],
   "source": [
    "\n",
    "net.eval()\n",
    "predicted_outputs, _ = net(x)\n",
    "\n",
    "# 画像をテンソルからNumPyに変化し、サイズを戻します\n",
    "pafs = predicted_outputs[0][0].detach().numpy().transpose(1, 2, 0)\n",
    "heatmaps = predicted_outputs[1][0].detach().numpy().transpose(1, 2, 0)\n",
    "\n",
    "pafs = cv2.resize(pafs, size, interpolation=cv2.INTER_CUBIC)\n",
    "heatmaps = cv2.resize(heatmaps, size, interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "pafs = cv2.resize(\n",
    "    pafs, (oriImg.shape[1], oriImg.shape[0]), interpolation=cv2.INTER_CUBIC)\n",
    "heatmaps = cv2.resize(\n",
    "    heatmaps, (oriImg.shape[1], oriImg.shape[0]), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "print('shape : (高さ, 幅, クラス数)')\n",
    "print('PAFsのshape : ' + str(pafs.shape))\n",
    "print('Confidence Mapのshape : ' + str(heatmaps.shape))\n",
    "\n",
    "# PAFsのクラス数はリンク19本のx,y方向のベクトル座標\n",
    "# Confidence Mapのクラス数は関節数（18個）といずれでもないクラスの合計\n",
    "# 出力配列の値は，各ピクセルが各クラスである確信度（≒確率）に対応"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### confidence mapとPAFsを可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cd-YecEqGgt5",
    "outputId": "2c992171-dd90-41ac-9332-707161bc0884"
   },
   "outputs": [],
   "source": [
    "# 左膝と左足首のconfidence map(heatmap)、そして左膝と左足首をつなぐPAFのxベクトルを可視化する\n",
    "# 左肘\n",
    "heat_map = heatmaps[:, :, 12]  # 6は左膝\n",
    "heat_map = Image.fromarray(np.uint8(cm.jet(heat_map)*255))\n",
    "heat_map = np.asarray(heat_map.convert('RGB'))\n",
    "\n",
    "# 合成して表示\n",
    "print('左膝のConfidence Map')\n",
    "blend_img = cv2.addWeighted(oriImg, 0.5, heat_map, 0.5, 0)\n",
    "plt.imshow(blend_img)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 左足首\n",
    "heat_map = heatmaps[:, :, 13]  # 7は左足首\n",
    "heat_map = Image.fromarray(np.uint8(cm.jet(heat_map)*255))\n",
    "heat_map = np.asarray(heat_map.convert('RGB'))\n",
    "\n",
    "# 合成して表示\n",
    "print('左足首のConfidence Map')\n",
    "blend_img = cv2.addWeighted(oriImg, 0.5, heat_map, 0.5, 0)\n",
    "plt.imshow(blend_img)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 左膝と左足首をつなぐPAFのxベクトル\n",
    "paf = pafs[:, :, 10]\n",
    "paf = Image.fromarray(np.uint8(cm.jet(paf)*255))\n",
    "paf = np.asarray(paf.convert('RGB'))\n",
    "\n",
    "# 合成して表示\n",
    "print('左膝と左足首をつなぐPAFのxベクトル')\n",
    "blend_img = cv2.addWeighted(oriImg, 0.5, paf, 0.5, 0)\n",
    "plt.imshow(blend_img)\n",
    "plt.show()\n",
    "\n",
    "# 左膝と左足首をつなぐPAFのyベクトル\n",
    "paf = pafs[:, :, 11]\n",
    "paf = Image.fromarray(np.uint8(cm.jet(paf)*255))\n",
    "paf = np.asarray(paf.convert('RGB'))\n",
    "\n",
    "# 合成して表示\n",
    "print('左膝と左足首をつなぐPAFのyベクトル')\n",
    "blend_img = cv2.addWeighted(oriImg, 0.5, paf, 0.5, 0)\n",
    "plt.imshow(blend_img)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 姿勢の構成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yhR2IvFEGgt5"
   },
   "outputs": [],
   "source": [
    "from utils.decode_pose import decode_pose\n",
    "_, result_img, _, _ = decode_pose(oriImg, heatmaps, pafs)\n",
    "#decode_pose : 各部位をつなぐリンクを求める関数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZFC-m6szGgt6",
    "outputId": "5b73a685-3801-4d49-d217-d8ba2afc78b0"
   },
   "outputs": [],
   "source": [
    "# 結果を描画\n",
    "plt.imshow(oriImg)\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(result_img)\n",
    "plt.savefig(\"HPE_result.png\")  #姿勢推定結果を保存\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a5nnUpdLGgt6"
   },
   "source": [
    "以上"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "4-7_OpenPose_inference.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.8.1 64-bit ('3.8.1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "196d5faedfcd5ce752f036a9957936f26ec314bb7bb5d3dbbe1fe7a54b537ace"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
